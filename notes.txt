Every actual test was done on Linux - Fedora 35  Kernel 5.16.12-200
GTX 1080 Driver Version 510.47.03 and X11

High learning rate seems to be a little wack
Removing the learning rate thing works fine, it's longer tho
You can also remove batch normalization

Accuracy after 50 epochs is around 70%-75%
using 65% for training time for convenience, and 65% is a good value
Need to use average to remove one off errors - 3 times is fine i think

Need to figure cross validation - this afternoon


So I am looking at:
optimizer function (SGD, Adam, Maybe RMSProp)
Learning rate (constant value vs dynamic // value)
Weight Initialization
Batch Normalization (maybe layer if i figure it out)
ReLU vs Leaky ReLU
Momentum (maybe)
Batch size

Original observation (same order as other list):
Very important, big difference
Constant is longer - high learning rate wack
Weight initialization doesn't seem to matter as much, but it seems more consistent with initialization steady increase all the time
Not sure about batch normalization
Leaky ReLU seems worst here
Haven't looked at the value for momentum
Batch size seems like a small improvment maybe? seems insignificant

Batch size = 64 -- Average 9:45 seconds ish


** Control test **
18:08 mminutes, did not converge to 65% (got close tho) -- Highest acc 64.27%
18:53 minutes, did not converge to 65% -- Highest acc 64.5%
It starts way lower than the others, which is strange maybe?
It definitely does not converge very fast
It is especially slow to recover from going in the wrong direction


I have weight decay and grad-clip on for everything, they keep things
consistent. I have widely varying training time otherwise. This helps
reduce outliers.

Let's not forget that Adam is not very different than adaptive learning rate.
Adam already does adaptive learning rate, so it's an interesting experiment to test
the effect of momentum on training time. But it's not perfect, since the adaptive
learning rate algorithm is not the same as the OneCyclePolicy, so little different

Normalization and initialization seems to have nice effects, but not on their own
Probably better with other methods.

Batch size doesn't to matter all that much, 200 is probably the best bet here.

After more testing, leaky relu sometimes seems like an improvment, but sometimes much slower
Overall, relu is more consistent than leaky relu. When leaky relu is faster, it is a minute faster,
when it is slower, it is several minutes slower. So I don't know which is better, probably relu

*** Best run yet, probably a one off, but ya know:
cuda
Epoch [0], test_loss: 2.3263, acc: 0.1658, Epoch_time: 0
Epoch [1], last_lr: 0.00069, train_loss: 1.7167, test_loss: 1.4641, acc: 0.4478, Epoch_time: 21.105917837998277
Epoch [2], last_lr: 0.00069, train_loss: 1.3431, test_loss: 1.3144, acc: 0.5005, Epoch_time: 21.021179268000196
Epoch [3], last_lr: 0.00069, train_loss: 1.1920, test_loss: 1.1977, acc: 0.5524, Epoch_time: 21.79563368900017
Epoch [4], last_lr: 0.00069, train_loss: 1.1099, test_loss: 1.1834, acc: 0.5621, Epoch_time: 20.19613980199938
Epoch [5], last_lr: 0.00069, train_loss: 1.0601, test_loss: 1.0942, acc: 0.5847, Epoch_time: 19.373024087000886
Epoch [6], last_lr: 0.00069, train_loss: 1.0243, test_loss: 1.2426, acc: 0.5508, Epoch_time: 20.868336543000623
Epoch [7], last_lr: 0.00069, train_loss: 0.9807, test_loss: 1.0843, acc: 0.6063, Epoch_time: 21.141124514000694
Epoch [8], last_lr: 0.00069, train_loss: 0.9440, test_loss: 1.0465, acc: 0.6123, Epoch_time: 21.29036162599914
Epoch [9], last_lr: 0.00069, train_loss: 0.9064, test_loss: 1.0288, acc: 0.6212, Epoch_time: 20.709238826999353
Epoch [10], last_lr: 0.00069, train_loss: 0.8948, test_loss: 1.0601, acc: 0.6094, Epoch_time: 20.80715393799983
Epoch [11], last_lr: 0.00069, train_loss: 0.8599, test_loss: 1.0270, acc: 0.6281, Epoch_time: 21.106598671998654
Epoch [12], last_lr: 0.00069, train_loss: 0.8393, test_loss: 1.1290, acc: 0.5859, Epoch_time: 20.986158485000487
Epoch [13], last_lr: 0.00069, train_loss: 0.8055, test_loss: 1.0619, acc: 0.6069, Epoch_time: 21.117637199000455
Epoch [14], last_lr: 0.00069, train_loss: 0.7940, test_loss: 0.9601, acc: 0.6520, Epoch_time: 21.172312468999735
0:04:52.709055

*** Hyperparameter search for learning rate:
Epoch [0], test_loss: 2.3263, acc: 0.1658, Epoch_time: 0
Epoch [1], last_lr: 0.00028, train_loss: 1.6530, test_loss: 1.4814, acc: 0.4457, Epoch_time: 19.72743010799968
Epoch [2], last_lr: 0.00028, train_loss: 1.3585, test_loss: 1.3136, acc: 0.4991, Epoch_time: 19.482787465
Epoch [3], last_lr: 0.00028, train_loss: 1.2268, test_loss: 1.3635, acc: 0.4908, Epoch_time: 19.015439267001057
Epoch [4], last_lr: 0.00028, train_loss: 1.1445, test_loss: 1.1614, acc: 0.5614, Epoch_time: 19.032677295999747
Epoch [5], last_lr: 0.00028, train_loss: 1.0789, test_loss: 1.1355, acc: 0.5681, Epoch_time: 19.030760789999476
Epoch [6], last_lr: 0.00028, train_loss: 1.0414, test_loss: 1.1348, acc: 0.5830, Epoch_time: 19.085799213000428
Epoch [7], last_lr: 0.00028, train_loss: 0.9935, test_loss: 1.0684, acc: 0.6014, Epoch_time: 19.099323525999353
Epoch [8], last_lr: 0.00028, train_loss: 0.9620, test_loss: 1.1187, acc: 0.5861, Epoch_time: 19.1196020210009
Epoch [9], last_lr: 0.00028, train_loss: 0.9213, test_loss: 1.0537, acc: 0.6087, Epoch_time: 19.10135526600061
Epoch [10], last_lr: 0.00028, train_loss: 0.8993, test_loss: 1.0314, acc: 0.6169, Epoch_time: 19.126491104998422
Epoch [11], last_lr: 0.00028, train_loss: 0.8701, test_loss: 1.0472, acc: 0.6157, Epoch_time: 19.10908100600136
Epoch [12], last_lr: 0.00028, train_loss: 0.8383, test_loss: 1.0979, acc: 0.5982, Epoch_time: 20.442538642000727
Epoch [13], last_lr: 0.00028, train_loss: 0.8087, test_loss: 1.0742, acc: 0.6027, Epoch_time: 20.56199927900161
Epoch [14], last_lr: 0.00028, train_loss: 0.7902, test_loss: 1.0464, acc: 0.6089, Epoch_time: 20.11819367100179
Epoch [15], last_lr: 0.00028, train_loss: 0.7579, test_loss: 1.0281, acc: 0.6295, Epoch_time: 20.14849063099973
Epoch [16], last_lr: 0.00028, train_loss: 0.7397, test_loss: 1.0263, acc: 0.6322, Epoch_time: 20.105266818998643
Epoch [17], last_lr: 0.00028, train_loss: 0.7118, test_loss: 1.0598, acc: 0.6203, Epoch_time: 20.269429847998254
Epoch [18], last_lr: 0.00028, train_loss: 0.6843, test_loss: 1.0795, acc: 0.6154, Epoch_time: 20.286904497999785
Epoch [19], last_lr: 0.00028, train_loss: 0.6689, test_loss: 1.1001, acc: 0.5929, Epoch_time: 20.604399004998413
Epoch [20], last_lr: 0.00028, train_loss: 0.6403, test_loss: 1.0187, acc: 0.6345, Epoch_time: 20.191379416999553
Epoch [21], last_lr: 0.00028, train_loss: 0.6136, test_loss: 0.9936, acc: 0.6407, Epoch_time: 20.212436188001448
Epoch [22], last_lr: 0.00028, train_loss: 0.5883, test_loss: 1.0884, acc: 0.6219, Epoch_time: 20.11518222199811
Epoch [23], last_lr: 0.00028, train_loss: 0.5661, test_loss: 1.0376, acc: 0.6386, Epoch_time: 20.154515448000893
Epoch [24], last_lr: 0.00028, train_loss: 0.5464, test_loss: 1.1230, acc: 0.6343, Epoch_time: 20.13913649800088
Epoch [25], last_lr: 0.00028, train_loss: 0.5226, test_loss: 1.0644, acc: 0.6300, Epoch_time: 20.204476923001494
Epoch [26], last_lr: 0.00028, train_loss: 0.5053, test_loss: 1.1330, acc: 0.6142, Epoch_time: 20.156994416996895
Epoch [27], last_lr: 0.00028, train_loss: 0.4822, test_loss: 1.1908, acc: 0.6108, Epoch_time: 20.13891375499952
Epoch [28], last_lr: 0.00028, train_loss: 0.4606, test_loss: 1.0595, acc: 0.6432, Epoch_time: 20.229631092999625
Epoch [29], last_lr: 0.00028, train_loss: 0.4322, test_loss: 1.0656, acc: 0.6456, Epoch_time: 20.17545085600068
Epoch [30], last_lr: 0.00028, train_loss: 0.4188, test_loss: 1.0596, acc: 0.6577, Epoch_time: 20.103414563000115
0:09:55.308847
cuda
Epoch [0], test_loss: 2.3263, acc: 0.1658, Epoch_time: 0
Epoch [1], last_lr: 0.00078, train_loss: 1.7525, test_loss: 1.4882, acc: 0.4364, Epoch_time: 20.317043205999653
Epoch [2], last_lr: 0.00078, train_loss: 1.3681, test_loss: 1.3036, acc: 0.4982, Epoch_time: 21.04292840099879
Epoch [3], last_lr: 0.00078, train_loss: 1.2081, test_loss: 1.2274, acc: 0.5391, Epoch_time: 20.311268002998986
Epoch [4], last_lr: 0.00078, train_loss: 1.1258, test_loss: 1.1273, acc: 0.5814, Epoch_time: 20.153881295998872
Epoch [5], last_lr: 0.00078, train_loss: 1.0675, test_loss: 1.1165, acc: 0.5724, Epoch_time: 20.08302672499849
Epoch [6], last_lr: 0.00078, train_loss: 1.0300, test_loss: 1.1599, acc: 0.5710, Epoch_time: 20.119806742000947
Epoch [7], last_lr: 0.00078, train_loss: 0.9851, test_loss: 1.0318, acc: 0.6226, Epoch_time: 20.143603055999847
Epoch [8], last_lr: 0.00078, train_loss: 0.9502, test_loss: 1.0573, acc: 0.6117, Epoch_time: 20.074512659000902
Epoch [9], last_lr: 0.00078, train_loss: 0.9112, test_loss: 1.0259, acc: 0.6223, Epoch_time: 20.16029558200171
Epoch [10], last_lr: 0.00078, train_loss: 0.8952, test_loss: 1.0407, acc: 0.6079, Epoch_time: 20.47405474599873
Epoch [11], last_lr: 0.00078, train_loss: 0.8711, test_loss: 1.0218, acc: 0.6308, Epoch_time: 20.150299175998953
Epoch [12], last_lr: 0.00078, train_loss: 0.8406, test_loss: 1.0322, acc: 0.6248, Epoch_time: 20.248950309000065
Epoch [13], last_lr: 0.00078, train_loss: 0.8144, test_loss: 1.0027, acc: 0.6268, Epoch_time: 20.121951403998537
Epoch [14], last_lr: 0.00078, train_loss: 0.7968, test_loss: 1.0060, acc: 0.6322, Epoch_time: 20.557110165002086
Epoch [15], last_lr: 0.00078, train_loss: 0.7684, test_loss: 0.9864, acc: 0.6439, Epoch_time: 21.19114933899982
Epoch [16], last_lr: 0.00078, train_loss: 0.7372, test_loss: 1.0690, acc: 0.6250, Epoch_time: 20.724305042000196
Epoch [17], last_lr: 0.00078, train_loss: 0.7220, test_loss: 1.0768, acc: 0.6129, Epoch_time: 20.439081304000865
Epoch [18], last_lr: 0.00078, train_loss: 0.6926, test_loss: 1.0982, acc: 0.6258, Epoch_time: 20.498731886000314
Epoch [19], last_lr: 0.00078, train_loss: 0.6598, test_loss: 1.1033, acc: 0.5860, Epoch_time: 20.659499989000324
Epoch [20], last_lr: 0.00078, train_loss: 0.6545, test_loss: 1.0218, acc: 0.6387, Epoch_time: 21.044702065999445
Epoch [21], last_lr: 0.00078, train_loss: 0.6231, test_loss: 1.0183, acc: 0.6405, Epoch_time: 20.822636518001673
Epoch [22], last_lr: 0.00078, train_loss: 0.6057, test_loss: 1.1571, acc: 0.6031, Epoch_time: 20.94595061200016
Epoch [23], last_lr: 0.00078, train_loss: 0.5721, test_loss: 1.0370, acc: 0.6530, Epoch_time: 20.45565720200102
0:07:50.760123
cuda
Epoch [0], test_loss: 2.3263, acc: 0.1658, Epoch_time: 0
Epoch [1], last_lr: 0.00029, train_loss: 1.6512, test_loss: 1.4855, acc: 0.4491, Epoch_time: 20.515162134001002
Epoch [2], last_lr: 0.00029, train_loss: 1.3534, test_loss: 1.2757, acc: 0.5215, Epoch_time: 20.004643966000003
Epoch [3], last_lr: 0.00029, train_loss: 1.2212, test_loss: 1.3259, acc: 0.5038, Epoch_time: 20.11944430299991
Epoch [4], last_lr: 0.00029, train_loss: 1.1478, test_loss: 1.1693, acc: 0.5650, Epoch_time: 20.092280494001898
Epoch [5], last_lr: 0.00029, train_loss: 1.0717, test_loss: 1.1258, acc: 0.5687, Epoch_time: 20.008248817001004
Epoch [6], last_lr: 0.00029, train_loss: 1.0348, test_loss: 1.1483, acc: 0.5774, Epoch_time: 20.062550204002036
Epoch [7], last_lr: 0.00029, train_loss: 0.9898, test_loss: 1.0809, acc: 0.5959, Epoch_time: 20.36145090899663
Epoch [8], last_lr: 0.00029, train_loss: 0.9577, test_loss: 1.0945, acc: 0.5956, Epoch_time: 20.75383473400143
Epoch [9], last_lr: 0.00029, train_loss: 0.9204, test_loss: 1.0764, acc: 0.5961, Epoch_time: 20.368027647000417
Epoch [10], last_lr: 0.00029, train_loss: 0.8987, test_loss: 1.0403, acc: 0.6099, Epoch_time: 20.18197229100042
Epoch [11], last_lr: 0.00029, train_loss: 0.8623, test_loss: 1.1128, acc: 0.5990, Epoch_time: 20.412127866002265
Epoch [12], last_lr: 0.00029, train_loss: 0.8319, test_loss: 1.0448, acc: 0.6175, Epoch_time: 20.220672265000758
Epoch [13], last_lr: 0.00029, train_loss: 0.8062, test_loss: 1.0804, acc: 0.6040, Epoch_time: 20.396935900000244
Epoch [14], last_lr: 0.00029, train_loss: 0.7850, test_loss: 1.0017, acc: 0.6328, Epoch_time: 20.090193432999513
Epoch [15], last_lr: 0.00029, train_loss: 0.7578, test_loss: 1.0457, acc: 0.6308, Epoch_time: 20.409326990997215
Epoch [16], last_lr: 0.00029, train_loss: 0.7324, test_loss: 1.0126, acc: 0.6347, Epoch_time: 19.91878399399866
Epoch [17], last_lr: 0.00029, train_loss: 0.7083, test_loss: 1.0203, acc: 0.6314, Epoch_time: 20.033235283000977
Epoch [18], last_lr: 0.00029, train_loss: 0.6858, test_loss: 1.0245, acc: 0.6326, Epoch_time: 20.157805101000122
Epoch [19], last_lr: 0.00029, train_loss: 0.6631, test_loss: 1.1066, acc: 0.5851, Epoch_time: 20.44426967199979
Epoch [20], last_lr: 0.00029, train_loss: 0.6326, test_loss: 1.0201, acc: 0.6367, Epoch_time: 20.543102018000354
Epoch [21], last_lr: 0.00029, train_loss: 0.6132, test_loss: 0.9904, acc: 0.6458, Epoch_time: 20.00430231899736
Epoch [22], last_lr: 0.00029, train_loss: 0.5878, test_loss: 1.0443, acc: 0.6355, Epoch_time: 20.2421301530012
Epoch [23], last_lr: 0.00029, train_loss: 0.5551, test_loss: 1.0768, acc: 0.6420, Epoch_time: 21.141836794999108
Epoch [24], last_lr: 0.00029, train_loss: 0.5440, test_loss: 1.1524, acc: 0.6273, Epoch_time: 20.786409788001038
Epoch [25], last_lr: 0.00029, train_loss: 0.5222, test_loss: 1.1469, acc: 0.6176, Epoch_time: 19.944455933000427
Epoch [26], last_lr: 0.00029, train_loss: 0.5003, test_loss: 1.0390, acc: 0.6333, Epoch_time: 20.38353100799941
Epoch [27], last_lr: 0.00029, train_loss: 0.4701, test_loss: 1.2352, acc: 0.5954, Epoch_time: 20.61460273099874
Epoch [28], last_lr: 0.00029, train_loss: 0.4533, test_loss: 1.0310, acc: 0.6489, Epoch_time: 20.27054631699866
Epoch [29], last_lr: 0.00029, train_loss: 0.4266, test_loss: 1.0770, acc: 0.6493, Epoch_time: 20.270157699000265
Epoch [30], last_lr: 0.00029, train_loss: 0.4127, test_loss: 1.1335, acc: 0.6418, Epoch_time: 19.997338422999746
Epoch [31], last_lr: 0.00029, train_loss: 0.3935, test_loss: 1.0763, acc: 0.6500, Epoch_time: 21.12531199300065
0:10:29.896110
cuda
Epoch [0], test_loss: 2.3263, acc: 0.1658, Epoch_time: 0
Epoch [1], last_lr: 0.00045, train_loss: 1.6698, test_loss: 1.4422, acc: 0.4578, Epoch_time: 20.583487383999454
Epoch [2], last_lr: 0.00045, train_loss: 1.3292, test_loss: 1.2693, acc: 0.5212, Epoch_time: 19.96827016699899
Epoch [3], last_lr: 0.00045, train_loss: 1.1948, test_loss: 1.3758, acc: 0.4974, Epoch_time: 20.34153958100069
Epoch [4], last_lr: 0.00045, train_loss: 1.1206, test_loss: 1.2874, acc: 0.5204, Epoch_time: 20.17282721200172
Epoch [5], last_lr: 0.00045, train_loss: 1.0639, test_loss: 1.1024, acc: 0.5862, Epoch_time: 20.489660786999593
Epoch [6], last_lr: 0.00045, train_loss: 1.0257, test_loss: 1.1177, acc: 0.5933, Epoch_time: 20.76413125399995
Epoch [7], last_lr: 0.00045, train_loss: 0.9727, test_loss: 1.0500, acc: 0.6068, Epoch_time: 20.681385062001937
Epoch [8], last_lr: 0.00045, train_loss: 0.9394, test_loss: 1.0517, acc: 0.6038, Epoch_time: 20.29897179099862
Epoch [9], last_lr: 0.00045, train_loss: 0.9010, test_loss: 1.0571, acc: 0.6044, Epoch_time: 19.98703811299856
Epoch [10], last_lr: 0.00045, train_loss: 0.8876, test_loss: 1.0157, acc: 0.6332, Epoch_time: 20.036270122000133
Epoch [11], last_lr: 0.00045, train_loss: 0.8544, test_loss: 0.9968, acc: 0.6356, Epoch_time: 20.007840808000765
Epoch [12], last_lr: 0.00045, train_loss: 0.8228, test_loss: 1.1029, acc: 0.5986, Epoch_time: 19.956040918998042
Epoch [13], last_lr: 0.00045, train_loss: 0.7977, test_loss: 1.1240, acc: 0.5905, Epoch_time: 19.950273525999364
Epoch [14], last_lr: 0.00045, train_loss: 0.7811, test_loss: 1.0048, acc: 0.6355, Epoch_time: 20.524752680998063
Epoch [15], last_lr: 0.00045, train_loss: 0.7450, test_loss: 0.9938, acc: 0.6389, Epoch_time: 20.33610499399947
Epoch [16], last_lr: 0.00045, train_loss: 0.7230, test_loss: 1.0418, acc: 0.6242, Epoch_time: 20.224808576000214
Epoch [17], last_lr: 0.00045, train_loss: 0.6972, test_loss: 0.9972, acc: 0.6409, Epoch_time: 20.093371145001583
Epoch [18], last_lr: 0.00045, train_loss: 0.6656, test_loss: 1.0917, acc: 0.6081, Epoch_time: 19.800545058998978
Epoch [19], last_lr: 0.00045, train_loss: 0.6507, test_loss: 1.1356, acc: 0.5829, Epoch_time: 19.872835976002534
Epoch [20], last_lr: 0.00045, train_loss: 0.6294, test_loss: 1.1413, acc: 0.6257, Epoch_time: 19.89607786600027
Epoch [21], last_lr: 0.00045, train_loss: 0.5971, test_loss: 1.0507, acc: 0.6302, Epoch_time: 19.917495033001615
Epoch [22], last_lr: 0.00045, train_loss: 0.5777, test_loss: 1.0543, acc: 0.6417, Epoch_time: 20.11071261699908
Epoch [23], last_lr: 0.00045, train_loss: 0.5419, test_loss: 1.0142, acc: 0.6490, Epoch_time: 21.12526424599855
Epoch [24], last_lr: 0.00045, train_loss: 0.5263, test_loss: 1.0484, acc: 0.6476, Epoch_time: 20.051094136000756
Epoch [25], last_lr: 0.00045, train_loss: 0.5065, test_loss: 0.9907, acc: 0.6542, Epoch_time: 20.063409014997887
0:08:25.273443
cuda
Epoch [0], test_loss: 2.3263, acc: 0.1658, Epoch_time: 0
Epoch [1], last_lr: 0.00053, train_loss: 1.6792, test_loss: 1.4032, acc: 0.4680, Epoch_time: 20.068527235998772
Epoch [2], last_lr: 0.00053, train_loss: 1.3261, test_loss: 1.3423, acc: 0.4883, Epoch_time: 20.004100631002075
Epoch [3], last_lr: 0.00053, train_loss: 1.1906, test_loss: 1.2861, acc: 0.5296, Epoch_time: 19.97259091599699
Epoch [4], last_lr: 0.00053, train_loss: 1.1202, test_loss: 1.2364, acc: 0.5423, Epoch_time: 20.709746739001275
Epoch [5], last_lr: 0.00053, train_loss: 1.0652, test_loss: 1.1113, acc: 0.5831, Epoch_time: 20.531919823999488
Epoch [6], last_lr: 0.00053, train_loss: 1.0248, test_loss: 1.1311, acc: 0.5781, Epoch_time: 20.12222597100117
Epoch [7], last_lr: 0.00053, train_loss: 0.9794, test_loss: 1.0651, acc: 0.6134, Epoch_time: 20.26176796100117
Epoch [8], last_lr: 0.00053, train_loss: 0.9405, test_loss: 1.0558, acc: 0.6059, Epoch_time: 20.06705717600198
Epoch [9], last_lr: 0.00053, train_loss: 0.9060, test_loss: 1.0684, acc: 0.6046, Epoch_time: 20.37316620099955
Epoch [10], last_lr: 0.00053, train_loss: 0.8908, test_loss: 1.0354, acc: 0.6187, Epoch_time: 20.53291019399694
Epoch [11], last_lr: 0.00053, train_loss: 0.8526, test_loss: 1.0009, acc: 0.6357, Epoch_time: 20.182933514999604
Epoch [12], last_lr: 0.00053, train_loss: 0.8349, test_loss: 1.1056, acc: 0.6016, Epoch_time: 19.863967560002493
Epoch [13], last_lr: 0.00053, train_loss: 0.7986, test_loss: 1.0573, acc: 0.6170, Epoch_time: 20.829231337000238
Epoch [14], last_lr: 0.00053, train_loss: 0.7830, test_loss: 1.0267, acc: 0.6348, Epoch_time: 21.101240227002563
Epoch [15], last_lr: 0.00053, train_loss: 0.7534, test_loss: 1.0015, acc: 0.6457, Epoch_time: 21.18338250799934
Epoch [16], last_lr: 0.00053, train_loss: 0.7290, test_loss: 1.0316, acc: 0.6361, Epoch_time: 21.2337055579992
Epoch [17], last_lr: 0.00053, train_loss: 0.7041, test_loss: 1.0103, acc: 0.6352, Epoch_time: 21.074515096999676
Epoch [18], last_lr: 0.00053, train_loss: 0.6759, test_loss: 1.0722, acc: 0.6250, Epoch_time: 21.521128878001036
Epoch [19], last_lr: 0.00053, train_loss: 0.6604, test_loss: 1.2052, acc: 0.5616, Epoch_time: 21.148977508000826
Epoch [20], last_lr: 0.00053, train_loss: 0.6371, test_loss: 1.0346, acc: 0.6383, Epoch_time: 21.166281734000222
Epoch [21], last_lr: 0.00053, train_loss: 0.6116, test_loss: 1.0309, acc: 0.6457, Epoch_time: 21.159962981997523
Epoch [22], last_lr: 0.00053, train_loss: 0.5866, test_loss: 1.0637, acc: 0.6427, Epoch_time: 21.122340338002687
Epoch [23], last_lr: 0.00053, train_loss: 0.5483, test_loss: 1.0193, acc: 0.6541, Epoch_time: 21.16874342200026
0:07:55.419325


** TEST WITH 0.00069 LEARNING RATE
Epoch [0], test_loss: 2.3263, acc: 0.1658, Epoch_time: 0
Epoch [1], last_lr: 0.00069, train_loss: 1.7170, test_loss: 1.4323, acc: 0.4531, Epoch_time: 21.31092435500068
Epoch [2], last_lr: 0.00069, train_loss: 1.3402, test_loss: 1.2874, acc: 0.5051, Epoch_time: 21.145126770999923
Epoch [3], last_lr: 0.00069, train_loss: 1.1846, test_loss: 1.2109, acc: 0.5479, Epoch_time: 21.178766693000398
Epoch [4], last_lr: 0.00069, train_loss: 1.1172, test_loss: 1.1918, acc: 0.5594, Epoch_time: 19.169393486000445
Epoch [5], last_lr: 0.00069, train_loss: 1.0681, test_loss: 1.0710, acc: 0.6017, Epoch_time: 19.116600456000015
Epoch [6], last_lr: 0.00069, train_loss: 1.0267, test_loss: 1.2332, acc: 0.5582, Epoch_time: 19.1089026489999
Epoch [7], last_lr: 0.00069, train_loss: 0.9797, test_loss: 1.1047, acc: 0.5936, Epoch_time: 19.125165467000443
Epoch [8], last_lr: 0.00069, train_loss: 0.9423, test_loss: 1.0555, acc: 0.6084, Epoch_time: 19.124104096000337
Epoch [9], last_lr: 0.00069, train_loss: 0.9107, test_loss: 1.0527, acc: 0.6105, Epoch_time: 20.623521599000014
Epoch [10], last_lr: 0.00069, train_loss: 0.8927, test_loss: 1.0014, acc: 0.6315, Epoch_time: 21.193706156999724
Epoch [11], last_lr: 0.00069, train_loss: 0.8613, test_loss: 0.9892, acc: 0.6351, Epoch_time: 20.45340577800016
Epoch [12], last_lr: 0.00069, train_loss: 0.8398, test_loss: 1.0542, acc: 0.6075, Epoch_time: 21.385526375000154
Epoch [13], last_lr: 0.00069, train_loss: 0.8077, test_loss: 1.0215, acc: 0.6209, Epoch_time: 20.619583366999905
Epoch [14], last_lr: 0.00069, train_loss: 0.7928, test_loss: 1.0161, acc: 0.6426, Epoch_time: 21.337958455000262
Epoch [15], last_lr: 0.00069, train_loss: 0.7617, test_loss: 0.9747, acc: 0.6474, Epoch_time: 20.694203137999466
Epoch [16], last_lr: 0.00069, train_loss: 0.7379, test_loss: 1.0713, acc: 0.6225, Epoch_time: 21.150635779000368
Epoch [17], last_lr: 0.00069, train_loss: 0.7109, test_loss: 1.0489, acc: 0.6168, Epoch_time: 20.624567269999716
Epoch [18], last_lr: 0.00069, train_loss: 0.6850, test_loss: 1.0394, acc: 0.6360, Epoch_time: 20.74563812499946
Epoch [19], last_lr: 0.00069, train_loss: 0.6671, test_loss: 1.1165, acc: 0.5982, Epoch_time: 20.68210701499993
Epoch [20], last_lr: 0.00069, train_loss: 0.6580, test_loss: 1.0125, acc: 0.6482, Epoch_time: 20.655561094000404
Epoch [21], last_lr: 0.00069, train_loss: 0.6133, test_loss: 1.0376, acc: 0.6424, Epoch_time: 20.751981745000194
Epoch [22], last_lr: 0.00069, train_loss: 0.6025, test_loss: 1.1776, acc: 0.6174, Epoch_time: 20.678246622999723
Epoch [23], last_lr: 0.00069, train_loss: 0.5621, test_loss: 1.0178, acc: 0.6610, Epoch_time: 20.73296568600017
0:07:51.628178
Epoch [0], test_loss: 3.6121, acc: 0.0147, Epoch_time: 0
Epoch [1], last_lr: 0.00069, train_loss: 1.7031, test_loss: 1.4742, acc: 0.4486, Epoch_time: 20.72699676299999
Epoch [2], last_lr: 0.00069, train_loss: 1.3224, test_loss: 1.3878, acc: 0.4732, Epoch_time: 20.745681745000184
Epoch [3], last_lr: 0.00069, train_loss: 1.1729, test_loss: 1.2733, acc: 0.5380, Epoch_time: 20.698704669000108
Epoch [4], last_lr: 0.00069, train_loss: 1.1077, test_loss: 1.1568, acc: 0.5759, Epoch_time: 21.10925310699986
Epoch [5], last_lr: 0.00069, train_loss: 1.0654, test_loss: 1.1260, acc: 0.5730, Epoch_time: 20.69591383199986
Epoch [6], last_lr: 0.00069, train_loss: 1.0047, test_loss: 1.2037, acc: 0.5650, Epoch_time: 20.797646451999753
Epoch [7], last_lr: 0.00069, train_loss: 0.9825, test_loss: 1.0412, acc: 0.6070, Epoch_time: 20.760971966999932
Epoch [8], last_lr: 0.00069, train_loss: 0.9426, test_loss: 1.0265, acc: 0.6269, Epoch_time: 20.73381947100006
Epoch [9], last_lr: 0.00069, train_loss: 0.9027, test_loss: 0.9865, acc: 0.6336, Epoch_time: 20.72767653000028
Epoch [10], last_lr: 0.00069, train_loss: 0.8769, test_loss: 1.0042, acc: 0.6252, Epoch_time: 20.69503918800001
Epoch [11], last_lr: 0.00069, train_loss: 0.8591, test_loss: 0.9944, acc: 0.6363, Epoch_time: 20.73008144400046
Epoch [12], last_lr: 0.00069, train_loss: 0.8279, test_loss: 1.0486, acc: 0.6217, Epoch_time: 20.854004410000016
Epoch [13], last_lr: 0.00069, train_loss: 0.7960, test_loss: 1.0168, acc: 0.6429, Epoch_time: 20.742904361
Epoch [14], last_lr: 0.00069, train_loss: 0.7763, test_loss: 1.0304, acc: 0.6328, Epoch_time: 20.80710329900012
Epoch [15], last_lr: 0.00069, train_loss: 0.7529, test_loss: 0.9898, acc: 0.6458, Epoch_time: 20.734022525999535
Epoch [16], last_lr: 0.00069, train_loss: 0.7265, test_loss: 1.0003, acc: 0.6397, Epoch_time: 20.715704764000293
Epoch [17], last_lr: 0.00069, train_loss: 0.7064, test_loss: 1.0051, acc: 0.6385, Epoch_time: 20.78348447099961
Epoch [18], last_lr: 0.00069, train_loss: 0.6769, test_loss: 1.0968, acc: 0.6257, Epoch_time: 20.76215529900037
Epoch [19], last_lr: 0.00069, train_loss: 0.6495, test_loss: 1.0505, acc: 0.6269, Epoch_time: 20.770197074999487
Epoch [20], last_lr: 0.00069, train_loss: 0.6305, test_loss: 1.0517, acc: 0.6368, Epoch_time: 20.77601802799927
Epoch [21], last_lr: 0.00069, train_loss: 0.6106, test_loss: 1.0367, acc: 0.6500, Epoch_time: 19.465236246999666
0:07:32.847221
Epoch [0], test_loss: 2.7604, acc: 0.1445, Epoch_time: 0
Epoch [1], last_lr: 0.00069, train_loss: 1.6984, test_loss: 1.5241, acc: 0.4201, Epoch_time: 20.010843281000234
Epoch [2], last_lr: 0.00069, train_loss: 1.3417, test_loss: 1.2522, acc: 0.5285, Epoch_time: 19.32339179600058
Epoch [3], last_lr: 0.00069, train_loss: 1.1870, test_loss: 1.2440, acc: 0.5211, Epoch_time: 19.22360964099971
Epoch [4], last_lr: 0.00069, train_loss: 1.1085, test_loss: 1.1379, acc: 0.5753, Epoch_time: 19.26819302399963
Epoch [5], last_lr: 0.00069, train_loss: 1.0580, test_loss: 1.1011, acc: 0.5959, Epoch_time: 20.25918624499991
Epoch [6], last_lr: 0.00069, train_loss: 1.0256, test_loss: 1.0812, acc: 0.6012, Epoch_time: 20.707872779000354
Epoch [7], last_lr: 0.00069, train_loss: 0.9764, test_loss: 1.0648, acc: 0.6036, Epoch_time: 19.355987050000294
Epoch [8], last_lr: 0.00069, train_loss: 0.9506, test_loss: 1.1198, acc: 0.5952, Epoch_time: 20.016821406999952
Epoch [9], last_lr: 0.00069, train_loss: 0.9248, test_loss: 1.1121, acc: 0.5875, Epoch_time: 20.056151034999857
Epoch [10], last_lr: 0.00069, train_loss: 0.8939, test_loss: 1.0652, acc: 0.6121, Epoch_time: 20.06250845700015
Epoch [11], last_lr: 0.00069, train_loss: 0.8663, test_loss: 0.9939, acc: 0.6371, Epoch_time: 20.02005218999966
Epoch [12], last_lr: 0.00069, train_loss: 0.8249, test_loss: 1.0581, acc: 0.5911, Epoch_time: 20.03989733399976
Epoch [13], last_lr: 0.00069, train_loss: 0.8142, test_loss: 1.0625, acc: 0.6192, Epoch_time: 20.08113986799981
Epoch [14], last_lr: 0.00069, train_loss: 0.7942, test_loss: 1.0980, acc: 0.6218, Epoch_time: 20.06483880899941
Epoch [15], last_lr: 0.00069, train_loss: 0.7574, test_loss: 1.0234, acc: 0.6270, Epoch_time: 20.050001168999188
Epoch [16], last_lr: 0.00069, train_loss: 0.7337, test_loss: 1.0063, acc: 0.6421, Epoch_time: 20.108267149000312
Epoch [17], last_lr: 0.00069, train_loss: 0.7152, test_loss: 1.0193, acc: 0.6373, Epoch_time: 20.1387667959998
Epoch [18], last_lr: 0.00069, train_loss: 0.6952, test_loss: 1.0777, acc: 0.6251, Epoch_time: 20.06397638300041
Epoch [19], last_lr: 0.00069, train_loss: 0.6637, test_loss: 1.0877, acc: 0.6280, Epoch_time: 20.134088924000025
Epoch [20], last_lr: 0.00069, train_loss: 0.6331, test_loss: 1.1205, acc: 0.6248, Epoch_time: 19.93602990999989
Epoch [21], last_lr: 0.00069, train_loss: 0.6305, test_loss: 1.0216, acc: 0.6390, Epoch_time: 19.475966390000394
Epoch [22], last_lr: 0.00069, train_loss: 0.6028, test_loss: 1.0300, acc: 0.6472, Epoch_time: 19.69703422299972
Epoch [23], last_lr: 0.00069, train_loss: 0.5725, test_loss: 1.0294, acc: 0.6495, Epoch_time: 19.613327754000238
Epoch [24], last_lr: 0.00069, train_loss: 0.5513, test_loss: 1.0473, acc: 0.6340, Epoch_time: 19.323387734999415
Epoch [25], last_lr: 0.00069, train_loss: 0.5279, test_loss: 1.0599, acc: 0.6603, Epoch_time: 19.650504924000415
0:08:16.730581

Average time: 7:54 seconds